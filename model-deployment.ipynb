{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b219d673",
   "metadata": {},
   "source": [
    "To deploy a model that is stored in mlflow, we have to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67bf651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e220e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path:str, test_path:str):\n",
    "    train = pd.read_csv(train_path, sep=\";\")\n",
    "    test = pd.read_csv(test_path, sep=\";\")\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "    \n",
    "def preprocess_data(train_data, test_data):\n",
    "\n",
    "    \n",
    "\n",
    "    trainX = train_data.drop(columns=[\"y\"])\n",
    "    trainy = train_data[\"y\"]\n",
    "\n",
    "    testX = test_data.drop(columns=[\"y\"])\n",
    "    testy = test_data[\"y\"]\n",
    "\n",
    "    trainy = trainy.replace({\"no\":0, \"yes\": 1})\n",
    "    testy = testy.replace({\"no\":0, \"yes\": 1})\n",
    "\n",
    "    trainX = trainX.to_dict(orient=\"records\")\n",
    "    testX = testX.to_dict(orient=\"records\")\n",
    "    \n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "\n",
    "def model_pipeline(trainX, trainy, testX, model_params:dict=None):\n",
    "\n",
    "    pipeline = make_pipeline(\n",
    "        DictVectorizer(),\n",
    "       DecisionTreeClassifier(**model_params)\n",
    "    )\n",
    "    \n",
    "    pipeline.fit(trainX, trainy)\n",
    "    \n",
    "    pred = pipeline.predict(testX)\n",
    "    \n",
    "    return pipeline, pred\n",
    "    \n",
    "\n",
    "def evaluate(testy, pred):\n",
    "    \n",
    "    return (f1_score(testy, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6803a03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44149577804583834\n"
     ]
    }
   ],
   "source": [
    "params = dict(max_depth=None, max_features=8, random_state=12, max_leaf_nodes=12)\n",
    "\n",
    "train, test = load_data(\"train.csv\", \"test.csv\")\n",
    "\n",
    "trainX, trainy, testX, testy = preprocess_data(train, test)\n",
    "\n",
    "pipeline, pred = model_pipeline(trainX, trainy, testX, params)\n",
    "\n",
    "print(evaluate(testy, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d2694",
   "metadata": {},
   "source": [
    "__Let's assume we have this very horrible model we want to log and deploy.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00d14a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='./mlruns/1', experiment_id='1', lifecycle_stage='active', name='term-deposit-exp', tags={}>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"term-deposit-exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d2cf989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44149577804583834\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    \n",
    "    mlflow.set_tag(\"developer\", \"Sharon\")\n",
    "    \n",
    "    mlflow.log_param(\"train-data\", \"train.csv\")\n",
    "    mlflow.log_param(\"test-data\", \"test.csv\")\n",
    "    \n",
    "    params = dict(max_depth=None, max_features=8, random_state=12, max_leaf_nodes=12)\n",
    "\n",
    "    train, test = load_data(\"train.csv\", \"test.csv\")\n",
    "\n",
    "    trainX, trainy, testX, testy = preprocess_data(train, test)\n",
    "\n",
    "    pipeline, pred = model_pipeline(trainX, trainy, testX, params)\n",
    "\n",
    "    score = evaluate(testy, pred)\n",
    "    \n",
    "    print(score)\n",
    "    \n",
    "    mlflow.log_metric(\"f1_score\", score)\n",
    "    \n",
    "    # log the model directly using .sklearn\n",
    "    mlflow.sklearn.log_model(pipeline, artifact_path=\"dtc-pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f15e95",
   "metadata": {},
   "source": [
    "<hr>\n",
    "To view any experiment, click on the time under the \"Start Time\" column depending on the experiment of your interest.\n",
    "\n",
    "<img src=\"images/experiments.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2bf6d4",
   "metadata": {},
   "source": [
    "### Deployment.\n",
    "\n",
    "1. Call the model from mlflow: After opening the experiment of your choice, scroll down a bit to \"Artifacts\". You should have something similiar to the image below.\n",
    "\n",
    ">Copy the full path:\n",
    "`./mlruns/1/a3a65c298163443a90bb0da817bc4b30/artifacts/dtc-pipeline`\n",
    "\n",
    "<img src=\"images/run_id.png\"/>\n",
    "\n",
    "2. Head over to VSCode and write some flask codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e83b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open your code editor, like VSCode.\n",
    "# In your terminal, active your environment, where you had earlier installed `mlflow`\n",
    "# Install Flask and pandas in your environment\n",
    "# Paste the code below, into your code editor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383198c5",
   "metadata": {},
   "source": [
    "```python\n",
    "1  import pandas as pd\n",
    "2 \n",
    "3  import mlflow\n",
    "4  from flask import Flask, request\n",
    "5\n",
    "6\n",
    "7  RUN_ID = \"a3a65c298163443a90bb0da817bc4b30\"\n",
    "8\n",
    "9  model = f'./mlruns/1/{RUN_ID}/artifacts/dtc-pipeline'\n",
    "10\n",
    "11 #Load model as a PyFuncModel.\n",
    "12 model = mlflow.pyfunc.load_model(model)\n",
    "13\n",
    "14\n",
    "15 app = Flask(\"term-deposit\")\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "\n",
    "    data = request.get_json()\n",
    "\n",
    "    pred = model.predict(data)\n",
    "\n",
    "    result = {\n",
    "        \"result\":int(pred)\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    app.run(debug=True, host=\"0.0.0.0\", port=\"8000\")\n",
    "    \n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c28946",
   "metadata": {},
   "source": [
    "__EXPLANATION__\n",
    "\n",
    "1. Replace `RUN_ID` with yours, same as the model path.\n",
    "2. Give the Flask name anything you prefer. It does not have to be `term-deposit`.\n",
    "3. The `/predict` route means that a user will be able to access this API when you open that route. For example: In GitHub, I cannot access any repository unless if I am on that route (`/repo_name`). So you can chage `/predict` to anything you want or use the base url (`/`).\n",
    "4. After defining the app route, we use the method `POST`. This means that a user of that route (endpoint) can post data for the backend to collect. The data in this contest will be what the model shall predict on. The default method is `GET`. This means a user cannot send us any data. It's useful if we only want to give info to users and not collect from them.\n",
    "5. Next is to define a function. We called it `predict`. This function is called anytime someone accesses this route. The finction gets the user data, which is in a json format and predicts the class. It returns the prediction as a json too.\n",
    "6. Lastly we define the port we want this entire API to run on. In our case it is: http://0.0.0.0/8000\n",
    "\n",
    "`To make a prediction will then be:` http://0.0.0.0/8000/predict as in the image below. \n",
    "\n",
    "Please note that using `localhost:` or `127.0.0.1:` inplace of `0.0.0.0/` would still work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8928e36",
   "metadata": {},
   "source": [
    "<img src=\"images/postman.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f9a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
